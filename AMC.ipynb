{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9729fef8",
   "metadata": {},
   "source": [
    "# Thought Process \n",
    "\n",
    "## Value Network\n",
    "\n",
    "This part accepts a board state and give us a score as the output. \n",
    "The simple implemtation used a basic function that compared the amount of \n",
    "pieces and give the value of the position based on that. \n",
    "\n",
    "However, in this case we have a neural network that takes in a position and gives an estimation on the \n",
    "value of the position. The idea is that the value network is going to work as a classifier giving perhaps something like a \n",
    "probility that white wins or a probabilty that black wins (can be represented with a negative probability perhaps)\n",
    "\n",
    "## Policy Network: \n",
    "This part accepts a board state as an input and gives a set of probabilities \n",
    "representing the probability of a move where higher probability means that the \n",
    "probability of the move leading to a dub is higher. \n",
    "\n",
    "## MCTS: \n",
    "Basic idea is to start with an empty tree, then use MCTS to build up a portion of the game tree by running \n",
    "a number of simulations, where each simulation adds a node to the tree.  \n",
    "\n",
    "\n",
    "## Notes: \n",
    "\n",
    "In the original implementaion we used a 2D list, which is fine, but here \n",
    "using a tensor using numpy is going to be much faster and thus we need to rerepresnt the \n",
    "board (the game state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a21ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import Engine\n",
    "\n",
    "NUM_MOVES = 8 ** 4 \n",
    "\n",
    "def move_to_index(mv: Engine.move) -> int:\n",
    "    return ((mv.start_row * 8 + mv.start_col) * 64) + (mv.end_row * 8 + mv.end_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetSmall(nn.Module):\n",
    "    \"\"\"150 K parameters. Fast enough for use at every tree node.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.flat  = nn.Flatten()\n",
    "        self.fc    = nn.Linear(64 * 8 * 8, NUM_MOVES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        return self.fc(x)  # raw logits\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    \"\"\"Deeper network (~2 M params) used only at leaf nodes → slower but OK.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 128, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(256)\n",
    "        self.flat  = nn.Flatten()\n",
    "        self.fc    = nn.Linear(256 * 8 * 8, 512)\n",
    "        self.val   = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return torch.tanh(self.val(x)).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce217e6c",
   "metadata": {},
   "source": [
    "# DualNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6cd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNet:\n",
    "    \"\"\"Handles inference & (pre)training for PolicyNetSmall + ValueNet.\"\"\"\n",
    "    def __init__(self, device: str | None = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = PolicyNetSmall().to(self.device)\n",
    "        self.value  = ValueNet().to(self.device)\n",
    "        self.opt = torch.optim.Adam(list(self.policy.parameters()) + list(self.value.parameters()), lr=1e-3)\n",
    "        self.kl  = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    #  inference\n",
    "    @torch.no_grad()\n",
    "    def predict(self, state: Engine.GameState) -> Tuple[Dict[Engine.move, float], float]:\n",
    "        board = torch.tensor(state.get_current_state(), dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        logits = self.policy(board).squeeze(0)\n",
    "        p_full = torch.softmax(logits, dim=0).cpu().numpy()\n",
    "        v = float(self.value(board).item())\n",
    "\n",
    "        legal_moves = state.get_all_valid_moves()\n",
    "        if not legal_moves:\n",
    "            return {}, v\n",
    "        move_probs: Dict[Engine.move, float] = {}\n",
    "        total = 0.0\n",
    "        for mv in legal_moves:\n",
    "            p = p_full[move_to_index(mv)]\n",
    "            move_probs[mv] = p\n",
    "            total += p\n",
    "        if total == 0:\n",
    "            move_probs = {m: 1/len(legal_moves) for m in legal_moves}\n",
    "        else:\n",
    "            move_probs = {m: p/total for m, p in move_probs.items()}\n",
    "        return move_probs, v\n",
    "\n",
    "    #  joint self‑play training (MCTS targets)\n",
    "    def train(self, batch: List[Tuple[np.ndarray, Dict[Engine.move, float], int]], epochs=1, batch_sz=64):\n",
    "        if not batch:\n",
    "            return\n",
    "        X, P, V = [], [], []\n",
    "        for board, pi, outcome in batch:\n",
    "            X.append(torch.tensor(board, dtype=torch.float32))\n",
    "            vec = np.zeros(NUM_MOVES, dtype=np.float32)\n",
    "            for mv, p in pi.items():\n",
    "                vec[move_to_index(mv)] = p\n",
    "            P.append(torch.tensor(vec, dtype=torch.float32))\n",
    "            V.append(float(outcome))\n",
    "        X = torch.stack(X).to(self.device)\n",
    "        P = torch.stack(P).to(self.device)\n",
    "        V = torch.tensor(V, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            perm = torch.randperm(len(batch), device=self.device)\n",
    "            X, P, V = X[perm], P[perm], V[perm]\n",
    "            for i in range(0, len(batch), batch_sz):\n",
    "                xb, pb, vb = X[i:i+batch_sz], P[i:i+batch_sz], V[i:i+batch_sz]\n",
    "                self.opt.zero_grad()\n",
    "                pol_logits = self.policy(xb)\n",
    "                val_pred   = self.value(xb)\n",
    "                loss_p = self.kl(F.log_softmax(pol_logits, dim=1), pb)\n",
    "                loss_v = self.mse(val_pred, vb)\n",
    "                (loss_p + loss_v).backward()\n",
    "                self.opt.step()\n",
    "\n",
    "    #  policy pretraining \n",
    "    def pretrain_policy(self, examples: List[Tuple[np.ndarray, np.ndarray]], epochs=3, batch_sz=256):\n",
    "        if not examples:\n",
    "            return\n",
    "        X = torch.stack([torch.tensor(b, dtype=torch.float32) for b, _ in examples]).to(self.device)\n",
    "        Y = torch.stack([torch.tensor(y, dtype=torch.float32) for _, y in examples]).to(self.device)\n",
    "        for _ in range(epochs):\n",
    "            perm = torch.randperm(len(examples), device=self.device)\n",
    "            X, Y = X[perm], Y[perm]\n",
    "            for i in range(0, len(examples), batch_sz):\n",
    "                xb, yb = X[i:i+batch_sz], Y[i:i+batch_sz]\n",
    "                self.opt.zero_grad()\n",
    "                logits = self.policy(xb)\n",
    "                loss = self.kl(F.log_softmax(logits, dim=1), yb)\n",
    "                loss.backward(); self.opt.step()\n",
    "\n",
    "    #  value pretraining \n",
    "    def pretrain_value(self, examples: List[Tuple[np.ndarray, float]], epochs=1, batch_sz=256):\n",
    "        if not examples:\n",
    "            return\n",
    "        X = torch.stack([torch.tensor(b, dtype=torch.float32) for b, _ in examples]).to(self.device)\n",
    "        y = torch.tensor([v for _, v in examples], dtype=torch.float32).to(self.device)\n",
    "        for _ in range(epochs):\n",
    "            perm = torch.randperm(len(examples), device=self.device)\n",
    "            X, y = X[perm], y[perm]\n",
    "            for i in range(0, len(examples), batch_sz):\n",
    "                xb, yb = X[i:i+batch_sz], y[i:i+batch_sz]\n",
    "                self.opt.zero_grad()\n",
    "                v_pred = self.value(xb)\n",
    "                loss = self.mse(v_pred, yb)\n",
    "                loss.backward(); self.opt.step()\n",
    "\n",
    "    #  persistence\n",
    "    def save(self, path_root=\"dualnet\"):\n",
    "        torch.save(self.policy.state_dict(), f\"{path_root}_policy.pth\")\n",
    "        torch.save(self.value.state_dict(),  f\"{path_root}_value.pth\")\n",
    "\n",
    "    def load(self, path_root=\"dualnet\"):\n",
    "        self.policy.load_state_dict(torch.load(f\"{path_root}_policy.pth\", map_location=self.device))\n",
    "        self.value.load_state_dict(torch.load(f\"{path_root}_value.pth\",  map_location=self.device))\n",
    "        self.policy.eval(); self.value.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69442e67",
   "metadata": {},
   "source": [
    "# Bootstrapping labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da997164",
   "metadata": {},
   "outputs": [],
   "source": [
    "PV = {\"p\":100, \"N\":320, \"B\":330, \"R\":500, \"Q\":900, \"K\":0}\n",
    "# Simple PSTs (centipawns). White tables; black tables are mirrored by row.\n",
    "PST_P = np.array([\n",
    "    [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
    "    [50, 50, 50, 50, 50, 50, 50, 50],\n",
    "    [10, 10, 20, 30, 30, 20, 10, 10],\n",
    "    [ 5,  5, 10, 25, 25, 10,  5,  5],\n",
    "    [ 0,  0,  0, 20, 20,  0,  0,  0],\n",
    "    [ 5, -5,-10,  0,  0,-10, -5,  5],\n",
    "    [ 5, 10, 10,-20,-20, 10, 10,  5],\n",
    "    [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
    "], dtype=np.int16)\n",
    "PST_N = np.array([\n",
    "    [-50,-40,-30,-30,-30,-30,-40,-50],\n",
    "    [-40,-20,  0,  0,  0,  0,-20,-40],\n",
    "    [-30,  0, 10, 15, 15, 10,  0,-30],\n",
    "    [-30,  5, 15, 20, 20, 15,  5,-30],\n",
    "    [-30,  0, 15, 20, 20, 15,  0,-30],\n",
    "    [-30,  5, 10, 15, 15, 10,  5,-30],\n",
    "    [-40,-20,  0,  5,  5,  0,-20,-40],\n",
    "    [-50,-40,-30,-30,-30,-30,-40,-50],\n",
    "], dtype=np.int16)\n",
    "PST_B = np.array([\n",
    "    [-20,-10,-10,-10,-10,-10,-10,-20],\n",
    "    [-10,  5,  0,  0,  0,  0,  5,-10],\n",
    "    [-10, 10, 10, 10, 10, 10, 10,-10],\n",
    "    [-10,  0, 10, 10, 10, 10,  0,-10],\n",
    "    [-10,  5,  5, 10, 10,  5,  5,-10],\n",
    "    [-10,  0,  5, 10, 10,  5,  0,-10],\n",
    "    [-10,  0,  0,  0,  0,  0,  0,-10],\n",
    "    [-20,-10,-10,-10,-10,-10,-10,-20],\n",
    "], dtype=np.int16)\n",
    "PST_R = np.array([\n",
    "    [ 0,  0,  0,  5,  5,  0,  0,  0],\n",
    "    [-5,  0,  0,  0,  0,  0,  0, -5],\n",
    "    [-5,  0,  0,  0,  0,  0,  0, -5],\n",
    "    [-5,  0,  0,  0,  0,  0,  0, -5],\n",
    "    [-5,  0,  0,  0,  0,  0,  0, -5],\n",
    "    [-5,  0,  0,  0,  0,  0,  0, -5],\n",
    "    [ 5, 10, 10, 10, 10, 10, 10,  5],\n",
    "    [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
    "], dtype=np.int16)\n",
    "PST_Q = np.array([\n",
    "    [-20,-10,-10, -5, -5,-10,-10,-20],\n",
    "    [-10,  0,  0,  0,  0,  0,  0,-10],\n",
    "    [-10,  0,  5,  5,  5,  5,  0,-10],\n",
    "    [ -5,  0,  5,  5,  5,  5,  0, -5],\n",
    "    [  0,  0,  5,  5,  5,  5,  0, -5],\n",
    "    [-10,  5,  5,  5,  5,  5,  0,-10],\n",
    "    [-10,  0,  5,  0,  0,  0,  0,-10],\n",
    "    [-20,-10,-10, -5, -5,-10,-10,-20],\n",
    "], dtype=np.int16)\n",
    "PST_K = np.array([\n",
    "    [ 20, 30, 10,  0,  0, 10, 30, 20],\n",
    "    [ 20, 20,  0,  0,  0,  0, 20, 20],\n",
    "    [-10,-20,-20,-20,-20,-20,-20,-10],\n",
    "    [-20,-30,-30,-40,-40,-30,-30,-20],\n",
    "    [-30,-40,-40,-50,-50,-40,-40,-30],\n",
    "    [-30,-40,-40,-50,-50,-40,-40,-30],\n",
    "    [-30,-40,-40,-50,-50,-40,-40,-30],\n",
    "    [-30,-40,-40,-50,-50,-40,-40,-30],\n",
    "], dtype=np.int16)\n",
    "PST_MAP = {\"p\": PST_P, \"N\": PST_N, \"B\": PST_B, \"R\": PST_R, \"Q\": PST_Q, \"K\": PST_K}\n",
    "\n",
    "class BootstrapPretrainer:\n",
    "    def __init__(self, temperature: float = 0.7):\n",
    "        self.t = max(temperature, 1e-3)\n",
    "\n",
    "    @staticmethod\n",
    "    def eval_board_white(board: List[List[str]]) -> int:\n",
    "        \"\"\"Static eval in centipawns from White's perspective.\"\"\"\n",
    "        score = 0\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                p = board[r][c]\n",
    "                if p == \"--\":\n",
    "                    continue\n",
    "                val = PV[p[1]]\n",
    "                pst = PST_MAP[p[1]][r, c] if p[0] == 'w' else PST_MAP[p[1]][7 - r, c]\n",
    "                sgn = 1 if p[0] == 'w' else -1\n",
    "                score += sgn * (val + pst)\n",
    "        return score\n",
    "\n",
    "    def policy_targets(self, state: Engine.GameState) -> Dict[Engine.move, float]:\n",
    "        \"\"\"Return a probability distribution over legal moves using softmax of\n",
    "        *evaluation improvement* for the side to move.\"\"\"\n",
    "        moves = state.get_all_valid_moves()\n",
    "        if not moves:\n",
    "            return {}\n",
    "        base = self.eval_board_white(state.board)\n",
    "        scores = []\n",
    "        for mv in moves:\n",
    "            s2 = copy.deepcopy(state)\n",
    "            s2.make_move(mv)\n",
    "            nxt = self.eval_board_white(s2.board)\n",
    "            diff = nxt - base\n",
    "            if not state.white_to_move:\n",
    "                diff = -diff  # from side-to-move perspective\n",
    "            scores.append(diff)\n",
    "        # softmax with temperature\n",
    "        x = np.array(scores, dtype=np.float32) / self.t\n",
    "        x -= x.max()\n",
    "        e = np.exp(x)\n",
    "        Z = e.sum()\n",
    "        if Z <= 0:\n",
    "            probs = np.full(len(moves), 1/len(moves), dtype=np.float32)\n",
    "        else:\n",
    "            probs = e / Z\n",
    "        return dict(zip(moves, probs.tolist()))\n",
    "\n",
    "    def generate_policy_dataset(self, n_positions: int = 5000, max_plies: int = 80, eps: float = 0.2) -> Tuple[List[Tuple[np.ndarray, np.ndarray]], List[Tuple[np.ndarray, float]]]:\n",
    "        \"\"\"Play many quick pseudo‑games with epsilon-greedy using the static eval\n",
    "        to collect (board, policy-distribution) pairs. Also returns optional\n",
    "        value targets derived from static eval (tanh-scaled) for value pretraining.\"\"\"\n",
    "        policy_examples: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        value_examples:  List[Tuple[np.ndarray, float]] = []\n",
    "        while len(policy_examples) < n_positions:\n",
    "            gs = Engine.GameState()\n",
    "            for _ in range(max_plies):\n",
    "                if gs.check_mate or gs.stale_mate:\n",
    "                    break\n",
    "                moves = gs.get_all_valid_moves()\n",
    "                if not moves:\n",
    "                    break\n",
    "                # build policy targets for current state\n",
    "                dist = self.policy_targets(gs)\n",
    "                vec = np.zeros(NUM_MOVES, dtype=np.float32)\n",
    "                for mv, p in dist.items():\n",
    "                    vec[move_to_index(mv)] = p\n",
    "                policy_examples.append((gs.get_current_state(), vec))\n",
    "                # optional value target from static eval scaled to [-1,1]\n",
    "                e = self.eval_board_white(gs.board)\n",
    "                v = np.tanh(e / 400.0)\n",
    "                value_examples.append((gs.get_current_state(), float(v)))\n",
    "                # choose action epsilon‑greedy from dist\n",
    "                if random.random() < eps:\n",
    "                    mv = random.choice(list(dist.keys()))\n",
    "                else:\n",
    "                    mv = max(dist.items(), key=lambda kv: kv[1])[0]\n",
    "                gs.make_move(mv)\n",
    "                if len(policy_examples) >= n_positions:\n",
    "                    break\n",
    "        return policy_examples, value_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d02ba7",
   "metadata": {},
   "source": [
    "# MCTS using dual net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    __slots__ = (\"state\",\"parent\",\"children\",\"visit_count\",\"total_value\",\"prior\",\"move\")\n",
    "    def __init__(self, state: Engine.GameState, parent: Optional['MCTSNode']=None,\n",
    "                 prior=0.0, move: Optional[Engine.move]=None):\n",
    "        self.state = state; self.parent = parent; self.prior = prior; self.move = move\n",
    "        self.children: Dict[Engine.move,'MCTSNode'] = {}\n",
    "        self.visit_count = 0; self.total_value = 0.0\n",
    "    def q(self):\n",
    "        return self.total_value / self.visit_count if self.visit_count else 0.0\n",
    "    def is_leaf(self):\n",
    "        return not self.children\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, net: DualNet, sims=200, c_puct=1.4):\n",
    "        self.net, self.N, self.c = net, sims, c_puct\n",
    "    def search(self, root_state: Engine.GameState) -> Dict[Engine.move,float]:\n",
    "        root = MCTSNode(root_state)\n",
    "        priors, _ = self.net.predict(root_state)\n",
    "        for mv,p in priors.items():\n",
    "            root.children[mv] = MCTSNode(self._next(root_state,mv), parent=root, prior=p, move=mv)\n",
    "        for _ in range(self.N):\n",
    "            leaf = self._select(root)\n",
    "            value = self._expand_eval(leaf)\n",
    "            self._backprop(leaf, value)\n",
    "        visits = np.array([ch.visit_count for ch in root.children.values()], dtype=np.float32)\n",
    "        if visits.sum()==0: return {}\n",
    "        probs = visits/visits.sum()\n",
    "        return dict(zip(root.children.keys(), probs))\n",
    "    def _select(self,n):\n",
    "        while not n.is_leaf():\n",
    "            best, best_s = None, -1e9; sqrt_p = np.sqrt(n.visit_count)\n",
    "            for ch in n.children.values():\n",
    "                u = self.c*ch.prior*sqrt_p/(1+ch.visit_count); s = ch.q()+u\n",
    "                if s>best_s: best,best_s=ch,s\n",
    "            n = best\n",
    "        return n\n",
    "    def _expand_eval(self,node):\n",
    "        t = self._term(node.state)\n",
    "        if t is not None: return t\n",
    "        priors,val = self.net.predict(node.state)\n",
    "        for mv,p in priors.items():\n",
    "            node.children[mv] = MCTSNode(self._next(node.state,mv), parent=node, prior=p, move=mv)\n",
    "        return val\n",
    "    def _backprop(self,node,val):\n",
    "        while node:\n",
    "            node.visit_count+=1; node.total_value+=val; val=-val; node=node.parent\n",
    "    @staticmethod\n",
    "    def _next(state,mv):\n",
    "        s = copy.deepcopy(state); s.make_move(mv); return s\n",
    "    @staticmethod\n",
    "    def _term(state):\n",
    "        if state.check_mate: return 1 if not state.white_to_move else -1\n",
    "        if state.stale_mate: return 0\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b36c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def self_play(net: DualNet, sims=300) -> List[Tuple[np.ndarray, Dict[Engine.move,float], int]]:\n",
    "    gs = Engine.GameState(); tree = MCTS(net, sims); hist=[]\n",
    "    while not (gs.check_mate or gs.stale_mate):\n",
    "        pi = tree.search(gs)\n",
    "        if not pi: break\n",
    "        hist.append((gs.get_current_state(), pi, None))\n",
    "        mv = max(pi.items(), key=lambda kv: kv[1])[0]\n",
    "        gs.make_move(mv)\n",
    "    z = 0\n",
    "    if gs.check_mate: z = 1 if not gs.white_to_move else -1\n",
    "    return [(b, p, z) for (b, p, _) in hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1524c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual = DualNet()\n",
    "\n",
    "# 1) Bootstrap pretraining so the policy is NOT random\n",
    "bootstrap = BootstrapPretrainer(temperature=0.7)\n",
    "pol_ds, val_ds = bootstrap.generate_policy_dataset(n_positions=3000, max_plies=80, eps=0.2)\n",
    "print(f\"Bootstrap: policy {len(pol_ds)}  value {len(val_ds)}\")\n",
    "dual.pretrain_policy(pol_ds, epochs=2)\n",
    "dual.pretrain_value(val_ds,  epochs=1)\n",
    "dual.save(\"dualnet_bootstrap\")\n",
    "\n",
    "# 2) Self‑play using the pretrained policy\n",
    "batch = self_play(dual, sims=80)\n",
    "print(f\"Self-play positions: {len(batch)}\")\n",
    "dual.train(batch, epochs=1)\n",
    "dual.save(\"dualnet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (fa_py310)",
   "language": "python",
   "name": "fa_py310"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
